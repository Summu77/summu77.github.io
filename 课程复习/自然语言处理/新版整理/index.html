<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../%E8%80%83%E8%AF%95%E8%AF%B4%E6%98%8E/ rel=prev><link href=../../../%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%A4%A7%E4%BD%9C%E4%B8%9A/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.2"><title>题型梳理 - Whu-Summu77</title><link rel=stylesheet href=../../../assets/stylesheets/main.50c56a3b.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link rel=stylesheet href=../../../assets/stylesheets/custom.00c04c01.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#_1 class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label=不再显示此消息> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> For updates contact <strong>@summu77</strong> on <strong>1875418876@qq.com</strong> </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../.. title=Whu-Summu77 class="md-header__button md-logo" aria-label=Whu-Summu77 data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Whu-Summu77 </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 题型梳理 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> 主页 </a> </li> <li class=md-tabs__item> <a href=../../../%E5%85%B3%E4%BA%8E%E6%88%91/Aboutme/ class=md-tabs__link> About me </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/%E6%9C%9F%E6%9C%AB%E6%A2%B3%E7%90%86/ class=md-tabs__link> 课程梳理 </a> </li> <li class=md-tabs__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Pytorch/ class=md-tabs__link> 深度学习基础 </a> </li> <li class=md-tabs__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/YOLO%E7%B3%BB%E5%88%97/ class=md-tabs__link> 深度学习应用 </a> </li> <li class=md-tabs__item> <a href=../../../%E4%BB%A3%E7%A0%81%E7%A0%94%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90/Pytorch/ class=md-tabs__link> 代码研读与分析 </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title=Whu-Summu77 class="md-nav__button md-logo" aria-label=Whu-Summu77 data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> Whu-Summu77 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> 主页 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> About me </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> About me </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../%E5%85%B3%E4%BA%8E%E6%88%91/Aboutme/ class=md-nav__link> <span class=md-ellipsis> Welcome </span> </a> </li> <li class=md-nav__item> <a href=../../../%E5%85%B3%E4%BA%8E%E6%88%91/%E6%95%B0%E6%A8%A1%E7%BB%8F%E5%8E%86/ class=md-nav__link> <span class=md-ellipsis> 数学建模 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E5%85%B3%E4%BA%8E%E6%88%91/%E7%A7%91%E7%A0%94%E7%BB%8F%E5%8E%86/ class=md-nav__link> <span class=md-ellipsis> 第一篇论文 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E5%85%B3%E4%BA%8E%E6%88%91/%E8%BE%A9%E8%AE%BA/ class=md-nav__link> <span class=md-ellipsis> 我与辩论 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E5%85%B3%E4%BA%8E%E6%88%91/%E5%BF%97%E6%84%BF%E5%AE%9E%E8%B7%B5/ class=md-nav__link> <span class=md-ellipsis> 社会实践 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> 课程梳理 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> 课程梳理 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex> <span class=md-ellipsis> 信息系统安全 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> 信息系统安全 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/%E6%9C%9F%E6%9C%AB%E6%A2%B3%E7%90%86/ class=md-nav__link> <span class=md-ellipsis> 期末梳理 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/CVE-2023-21839%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/ class=md-nav__link> <span class=md-ellipsis> CVE21839复现 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> 计算机网络 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> 计算机网络 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%AC%E4%B8%80%E7%AB%A0/ class=md-nav__link> <span class=md-ellipsis> 1-基础概要复习 </span> </a> </li> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%AC%E4%BA%8C%E7%AB%A0/ class=md-nav__link> <span class=md-ellipsis> 2-物理层复习 </span> </a> </li> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%AC%E4%B8%89%E7%AB%A0/ class=md-nav__link> <span class=md-ellipsis> 3-链路层复习 </span> </a> </li> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%AC%E5%9B%9B%E7%AB%A0/ class=md-nav__link> <span class=md-ellipsis> 4-网络层复习 </span> </a> </li> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%AC%E4%BA%94%E7%AB%A0/ class=md-nav__link> <span class=md-ellipsis> 5-传输层复习 </span> </a> </li> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%AC%E5%85%AD%E7%AB%A0/ class=md-nav__link> <span class=md-ellipsis> 6-应用层复习 </span> </a> </li> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%A2%84%E6%B5%8B/ class=md-nav__link> <span class=md-ellipsis> 试卷分析 </span> </a> </li> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BA%8C%E8%BD%AE%E5%A4%8D%E4%B9%A0/ class=md-nav__link> <span class=md-ellipsis> 二轮复习 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_3 checked> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> 自然语言处理 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> 自然语言处理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../%E8%80%83%E8%AF%95%E8%AF%B4%E6%98%8E/ class=md-nav__link> <span class=md-ellipsis> 考试说明 </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 题型梳理 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 题型梳理 </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 重要的模型！ </span> </a> <nav class=md-nav aria-label=重要的模型！> <ul class=md-nav__list> <li class=md-nav__item> <a href=#word2vec class=md-nav__link> <span class=md-ellipsis> Word2vec模型 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 前馈神经网络 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 卷积神经网络 </span> </a> </li> <li class=md-nav__item> <a href=#rnn-lstm-gru class=md-nav__link> <span class=md-ellipsis> RNN LSTM GRU模型 </span> </a> </li> <li class=md-nav__item> <a href=#transformer class=md-nav__link> <span class=md-ellipsis> Transformer的架构 </span> </a> </li> <li class=md-nav__item> <a href=#bert class=md-nav__link> <span class=md-ellipsis> BERT的架构 </span> </a> </li> <li class=md-nav__item> <a href=#gpt-1 class=md-nav__link> <span class=md-ellipsis> GPT-1的架构 </span> </a> </li> <li class=md-nav__item> <a href=#seqtoseq class=md-nav__link> <span class=md-ellipsis> Seqtoseq的架构 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 名词解释题 </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 简答题 </span> </a> <nav class=md-nav aria-label=简答题> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> 句法成分树 </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> 依存树(图) </span> </a> </li> <li class=md-nav__item> <a href=#rst class=md-nav__link> <span class=md-ellipsis> 修辞结构理论(RST) </span> </a> </li> <li class=md-nav__item> <a href=#_9 class=md-nav__link> <span class=md-ellipsis> 注意力机制 </span> </a> </li> <li class=md-nav__item> <a href=#_10 class=md-nav__link> <span class=md-ellipsis> 其他部分 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_11 class=md-nav__link> <span class=md-ellipsis> 算法题 </span> </a> <nav class=md-nav aria-label=算法题> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_12 class=md-nav__link> <span class=md-ellipsis> 分词算法 </span> </a> </li> <li class=md-nav__item> <a href=#_13 class=md-nav__link> <span class=md-ellipsis> 词性标注算法 </span> </a> </li> <li class=md-nav__item> <a href=#_14 class=md-nav__link> <span class=md-ellipsis> 生成模型解码算法 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_15 class=md-nav__link> <span class=md-ellipsis> 设计题 </span> </a> <nav class=md-nav aria-label=设计题> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_16 class=md-nav__link> <span class=md-ellipsis> 完整流程 </span> </a> </li> <li class=md-nav__item> <a href=#_17 class=md-nav__link> <span class=md-ellipsis> 评估指标 </span> </a> </li> <li class=md-nav__item> <a href=#_18 class=md-nav__link> <span class=md-ellipsis> 关系抽取 </span> </a> </li> <li class=md-nav__item> <a href=#_19 class=md-nav__link> <span class=md-ellipsis> 命名实体识别 </span> </a> </li> <li class=md-nav__item> <a href=#_20 class=md-nav__link> <span class=md-ellipsis> 主题情感分类 </span> </a> </li> <li class=md-nav__item> <a href=#_21 class=md-nav__link> <span class=md-ellipsis> 自然语言生成系统 </span> </a> <nav class=md-nav aria-label=自然语言生成系统> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_22 class=md-nav__link> <span class=md-ellipsis> 翻译系统 </span> </a> </li> <li class=md-nav__item> <a href=#_23 class=md-nav__link> <span class=md-ellipsis> 摘要系统 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4 id=__nav_3_4_label tabindex> <span class=md-ellipsis> 操作系统 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> 操作系统 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%A4%A7%E4%BD%9C%E4%B8%9A/ class=md-nav__link> <span class=md-ellipsis> 设计简陋的OS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5> <label class=md-nav__link for=__nav_3_5 id=__nav_3_5_label tabindex> <span class=md-ellipsis> 计算机病毒 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_5_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> 计算机病毒 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%97%85%E6%AF%92/%E9%92%93%E9%B1%BC%E9%82%AE%E4%BB%B6%E7%BB%BC%E8%BF%B0/ class=md-nav__link> <span class=md-ellipsis> 钓鱼邮件综述 </span> </a> </li> <li class=md-nav__item> <a href=../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%97%85%E6%AF%92/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%97%85%E6%AF%92%E6%9C%9F%E6%9C%AB/ class=md-nav__link> <span class=md-ellipsis> 期末复习 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> 深度学习基础 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> 深度学习基础 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_1> <label class=md-nav__link for=__nav_4_1 id=__nav_4_1_label tabindex> <span class=md-ellipsis> Python常用库 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_1_label aria-expanded=false> <label class=md-nav__title for=__nav_4_1> <span class="md-nav__icon md-icon"></span> Python常用库 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Pytorch/ class=md-nav__link> <span class=md-ellipsis> (!) Pytorch库 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Transformers/ class=md-nav__link> <span class=md-ellipsis> (!) Transformers库 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Diffusers/ class=md-nav__link> <span class=md-ellipsis> (!) Diffusers库 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Datasets/ class=md-nav__link> <span class=md-ellipsis> Datasets库 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas & Numpy库 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/matplotlib/ class=md-nav__link> <span class=md-ellipsis> Matplotlib & Seaborn库 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E9%A1%B9%E7%9B%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/ class=md-nav__link> <span class=md-ellipsis> 项目环境搭建 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8/ class=md-nav__link> <span class=md-ellipsis> 服务器使用 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/ class=md-nav__link> <span class=md-ellipsis> 零碎知识补充 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Normalization/ class=md-nav__link> <span class=md-ellipsis> Normalization </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> 深度学习应用 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> 深度学习应用 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_1> <label class=md-nav__link for=__nav_5_1 id=__nav_5_1_label tabindex> <span class=md-ellipsis> 计算机视觉 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_1_label aria-expanded=false> <label class=md-nav__title for=__nav_5_1> <span class="md-nav__icon md-icon"></span> 计算机视觉 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/YOLO%E7%B3%BB%E5%88%97/ class=md-nav__link> <span class=md-ellipsis> YOLO系列 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/DERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> DERT代码分析 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex> <span class=md-ellipsis> 生成模型 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> 生成模型 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/Diffusion%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> 生成模型底层原理 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/DDIM-DDPM/ class=md-nav__link> <span class=md-ellipsis> DDIM<-DDPM </span> </a> </li> <li class=md-nav__item> <a href=../../../%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/DDIM-inversion%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> DDIM Inversion </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%B8%B8%E8%A7%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/ class=md-nav__link> <span class=md-ellipsis> 常见大模型 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/ class=md-nav__link> <span class=md-ellipsis> PEFT:微调总结 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/ class=md-nav__link> <span class=md-ellipsis> 位置编码总结 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex> <span class=md-ellipsis> 代码研读与分析 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> 代码研读与分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../%E4%BB%A3%E7%A0%81%E7%A0%94%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90/Pytorch/ class=md-nav__link> <span class=md-ellipsis> (!)Pytorch基本功 </span> </a> </li> <li class=md-nav__item> <a href=../../../%E4%BB%A3%E7%A0%81%E7%A0%94%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90/DDPM%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> DDPM </span> </a> </li> <li class=md-nav__item> <a href=../../../%E4%BB%A3%E7%A0%81%E7%A0%94%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90/DDIM%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> DDIM </span> </a> </li> <li class=md-nav__item> <a href=../../../%E4%BB%A3%E7%A0%81%E7%A0%94%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90/LDM%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> LDM </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/VIT%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> VIT/DeiT </span> </a> </li> <li class=md-nav__item> <a href=../../../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/GPT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> GPT </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 重要的模型！ </span> </a> <nav class=md-nav aria-label=重要的模型！> <ul class=md-nav__list> <li class=md-nav__item> <a href=#word2vec class=md-nav__link> <span class=md-ellipsis> Word2vec模型 </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 前馈神经网络 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 卷积神经网络 </span> </a> </li> <li class=md-nav__item> <a href=#rnn-lstm-gru class=md-nav__link> <span class=md-ellipsis> RNN LSTM GRU模型 </span> </a> </li> <li class=md-nav__item> <a href=#transformer class=md-nav__link> <span class=md-ellipsis> Transformer的架构 </span> </a> </li> <li class=md-nav__item> <a href=#bert class=md-nav__link> <span class=md-ellipsis> BERT的架构 </span> </a> </li> <li class=md-nav__item> <a href=#gpt-1 class=md-nav__link> <span class=md-ellipsis> GPT-1的架构 </span> </a> </li> <li class=md-nav__item> <a href=#seqtoseq class=md-nav__link> <span class=md-ellipsis> Seqtoseq的架构 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 名词解释题 </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 简答题 </span> </a> <nav class=md-nav aria-label=简答题> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> 句法成分树 </span> </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> 依存树(图) </span> </a> </li> <li class=md-nav__item> <a href=#rst class=md-nav__link> <span class=md-ellipsis> 修辞结构理论(RST) </span> </a> </li> <li class=md-nav__item> <a href=#_9 class=md-nav__link> <span class=md-ellipsis> 注意力机制 </span> </a> </li> <li class=md-nav__item> <a href=#_10 class=md-nav__link> <span class=md-ellipsis> 其他部分 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_11 class=md-nav__link> <span class=md-ellipsis> 算法题 </span> </a> <nav class=md-nav aria-label=算法题> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_12 class=md-nav__link> <span class=md-ellipsis> 分词算法 </span> </a> </li> <li class=md-nav__item> <a href=#_13 class=md-nav__link> <span class=md-ellipsis> 词性标注算法 </span> </a> </li> <li class=md-nav__item> <a href=#_14 class=md-nav__link> <span class=md-ellipsis> 生成模型解码算法 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_15 class=md-nav__link> <span class=md-ellipsis> 设计题 </span> </a> <nav class=md-nav aria-label=设计题> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_16 class=md-nav__link> <span class=md-ellipsis> 完整流程 </span> </a> </li> <li class=md-nav__item> <a href=#_17 class=md-nav__link> <span class=md-ellipsis> 评估指标 </span> </a> </li> <li class=md-nav__item> <a href=#_18 class=md-nav__link> <span class=md-ellipsis> 关系抽取 </span> </a> </li> <li class=md-nav__item> <a href=#_19 class=md-nav__link> <span class=md-ellipsis> 命名实体识别 </span> </a> </li> <li class=md-nav__item> <a href=#_20 class=md-nav__link> <span class=md-ellipsis> 主题情感分类 </span> </a> </li> <li class=md-nav__item> <a href=#_21 class=md-nav__link> <span class=md-ellipsis> 自然语言生成系统 </span> </a> <nav class=md-nav aria-label=自然语言生成系统> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_22 class=md-nav__link> <span class=md-ellipsis> 翻译系统 </span> </a> </li> <li class=md-nav__item> <a href=#_23 class=md-nav__link> <span class=md-ellipsis> 摘要系统 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=_1>题型考点梳理</h1> <blockquote> <p>NLP 复习可以直接按照题型进行复习 </p> </blockquote> <h2 id=_2>重要的模型！</h2> <h3 id=word2vec>Word2vec模型</h3> <p>Word2vec 可以利用两种架构来生成：<strong>Continuous bag-of-words(CBOW)</strong> 和 <strong>Continuous skip-gram</strong> 。其核心思想是利用到了滑动窗口，每个窗口中的中间词称之为target word，而左右两侧的词称之为context word，前者使用的是上下文词来预测目标词，后者则是利用目标词来预测上下文词。</p> <div style="display: flex; justify-content: space-between;"> <figure class=figure-image style="text-align: center; width: 46%;"> <img src=../pics/image-7.png alt="An image caption" style="width: 100%; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <figure class=figure-image style="text-align: center; width: 47%;"> <img src=../pics/image-9.png alt="An image caption" style="width: 100%; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> </div> <blockquote> <p>负采样：不对全部的词表进行softmax，而是采样一些词进行softmax大大减少计算量</p> <p>非固定滑动窗口：实际上滑动窗口的大小是随机采样得到的，这样可以更好地利用离target更近的词</p> <p>sub-sampling ：认为低概率的词语义信息更加丰富，用策略去掉一些高概率词</p> </blockquote> <h3 id=_3>前馈神经网络</h3> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-14.png alt="An image caption" style="width: 120%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>训练目标：最小化代价函数 J ( W , b )，其中 W 和 b 分别表示网络中的权重矩阵和偏置向量</p> <p>参数更新：通过反向传播，一般使用梯度下降法(批梯度下降、随机梯度下降等)</p> </blockquote> <h3 id=_4>卷积神经网络</h3> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-21.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>优点：卷积层级之间的神经元是<strong>局部连接和权值共享</strong>，这样的设计大大减少了(w,b)的数量，加快了训练。</p> <p>卷积操作：从输入图像中提取特征，如边缘、纹理、颜色等，同时可以捕捉到平移不变性</p> <p>池化操作：降低卷积层的输出维度，减少计算复杂性，有助于保留显著的特征降低噪声的影响。</p> <p>文本卷积：使用的<strong>卷积核通常是一维的</strong>。这与用于图像处理的二维卷积核不同。卷积核的一维结构更适合处理线性排列的词向量序列。</p> </blockquote> <p>理解一下TextCNN的结构，它是二维的卷积核：</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-42.png alt="An image caption" style="width: 50%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <h3 id=rnn-lstm-gru>RNN LSTM GRU模型</h3> <blockquote> <p>参考资料：<a href="https://www.bilibili.com/video/BV1YK411F7Tg/?spm_id_from=333.337.search-card.all.click&vd_source=ae67b970bd4a0665fa92195df95aa1f3">数之道 09 RNN LSTM GRU</a></p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-24.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>RNN 会产生梯度消失和梯度爆炸的问题，且难以捕捉长期依赖关系。</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-28.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>依次是：遗忘门、输入门、输出门。解决了长期依赖的关系但是结构复杂计算成本高。</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-29.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-27.png alt="An image caption" style="width: 90%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>更新门（update gate），控制了新信息被加入到细胞状态的程度、重置门（reset gate），控制了过去的记忆信息被遗忘的程度。比LSTM更加简单，参数更少。</p> </blockquote> <h3 id=transformer>Transformer的架构</h3> <blockquote> <p>参考资料 ：清华刘知远老师的大模型课程自行整理，这个笔记为两个月前的，但是够用了</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image.jpg alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-22.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p><strong>Layer Normalization</strong>：批归一化适用于深度神经网络如CNN，层归一化更合适RNN和transformer。具体来说，Layer Normalization 对的是每一层的输入进行归一化，而不是对输入序列的样本或特征维度进行归一化。</p> <p><strong>Multihead Attention：</strong>每个头学习序列的不同方面，增加了模型的表达能力，<strong>允许模型同时关注序列的不同特征</strong>，如语法和语义层面。和多卷积核在图像上的作用类似。这些多个头的输出通常通过拼接或加权平均等方式来组合，以生成最终的注意力输出。</p> </blockquote> <h3 id=bert>BERT的架构</h3> <blockquote> <p>参考资料：<a href="https://www.bilibili.com/video/BV1ca411375k/?spm_id_from=333.788&vd_source=ae67b970bd4a0665fa92195df95aa1f3">水论文的程序猿: BERT与GPT</a></p> <p>BERT和transformer的encoder部分基本一样，但是层数变成了12或24层，输入层多了一个segmnet embedding，同时预训练任务变成了MLM和NSP。其最大的优点是<strong>上下文感知性</strong>！</p> <p>注意：ELMo虽然也是双向编码的，但是它在感知上文时不能感知下文，感知下文时不能感知上文</p> <p>和GPT一样，BERT也采用<strong>二段式训练</strong>方法：第一阶段：使用易获取的大规模无标签语料，来训练基础语言模型（即MLM和NSP）；第二阶段：根据指定任务的少量带标签训练数据进行微调训练。比如：自然语言推理、情感分类、问答、序列标注</p> </blockquote> <p><strong>MLM</strong>：每个序列中 15% 的单词被替换为 [MASK] 标记。然后，该模型尝试根据序列中其他非掩码单词提供的上下文来预测掩码单词的原始值。</p> <p><strong>NSP</strong>：接收成对的句子作为输入，学习预测该对中的第二个句子是否是第一个句子的后续句子。其目的是为了获得句子的语义联系，毕竟MLM只能学到词与词之间联系的。</p> <p><strong>输入层</strong>：比transformer的输入层多出一个segmnet embedding层，具体如下所示：</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-18.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>BERT本质是提取文本的特征，可以用于构建各种模型，完成各种任务！非常重要！</p> <p>模型设计甚至全都可以采取的思路：将BERT和其他简单模型组合</p> <p>拓展资料 ：<a href=https://www.cnblogs.com/nickchen121/p/15114385.html#%E5%8D%81%E4%B8%80bert-%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%94%B9%E9%80%A0>BERT 下游任务改造</a> ，<a href="https://www.bilibili.com/video/BV1se411V713/?spm_id_from=333.788&vd_source=ae67b970bd4a0665fa92195df95aa1f3">BERT 下游任务改造 视频版</a></p> </blockquote> <h3 id=gpt-1>GPT-1的架构</h3> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-19.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <p>GPT-1的训练：</p> <ol> <li>基于<strong>无监督学习</strong>，在训练过程中，GPT-1尝试预测给定文本序列中的下一个单词，通过阅读大量文本数据来学习语言的统计规律。</li> <li>进行<strong>有监督微调</strong>来提高其在特定任务上的性能，使用了一系列有标签的任务来微调模型，包括文本分类、命名实体识别、情感分析等。</li> </ol> <h3 id=seqtoseq>Seqtoseq的架构</h3> <blockquote> <p>Bert是预训练模型，期望在海量数据上学习理解字词的通用语义，再灌给下游各种个性化任务应用，而Transformer更多是类似于CNN，RNN的网络模型，直接端到端学习各种任务和样本，每个任务从头到尾建模。<strong>只有Bert和GPT可以预训练！transformer是不行的！</strong></p> <p>经过预训练之后，对于下游任务而言</p> <ol> <li> <p>如果是<strong>非自然语言生成任务</strong>，可以在Bert的输出层做一些改变就能很好适应，比如：命名实体识别、关系抽取、主题情感分类</p> </li> <li> <p>如果是<strong>自然语言生成的任务</strong>，可以使用Transformer的Decoder部分，将输入的文本序列转换为输出的文本序列。</p> </li> </ol> </blockquote> <h2 id=_5>名词解释题</h2> <table> <thead> <tr> <th>名词</th> <th>解释</th> <th>是否考过</th> </tr> </thead> <tbody> <tr> <td>分词</td> <td>将文本分割成更小的语言单位（如单词、短语或符号）的过程。在不同语言中，分词的方法和难度各不相同。在英语中通常是按空格和标点分割，而在汉语中由于没有明显的词界标记，分词更为复杂。</td> <td>✔️ ✔️</td> </tr> <tr> <td>词性标注</td> <td>为文本中的每个单词（或词语）标注词性（如名词、动词、形容词等）。这有助于了解词语在句子中的语法作用和意义。</td> <td>✔️</td> </tr> <tr> <td>词义排歧</td> <td>确定某多义词在特定上下文中的准确意义。这对于理解句子的意义很重要。</td> <td></td> </tr> <tr> <td>词向量</td> <td>将词语表示为多维空间中的向量的技术。这些向量能够捕捉词语之间的语义和语法关系，使计算机能够理解和处理自然语言。</td> <td>✔️ ✔️</td> </tr> <tr> <td>语言模型</td> <td>用来预测文本序列中的下一个词或词组的模型。这些模型基于大量语料库学习如何构建语言结构，用于各种自然语言处理任务，如文本生成、机器翻译等。</td> <td>✔️ ✔️</td> </tr> <tr> <td>关系抽取</td> <td><strong>从文本中识别并提取实体之间的关系的过程。</strong></td> <td></td> </tr> <tr> <td>实体识别</td> <td>从文本中识别出特定类型的实体（如人名、地点、组织名等）。</td> <td>✔️</td> </tr> <tr> <td>Aspect情感分析</td> <td><strong>识别文本中特定方面或属性的情感倾向。</strong>例如评价一个餐厅时，分别分析对于“食物”、“服务”、“环境”的情感倾向。</td> <td></td> </tr> <tr> <td>预训练</td> <td>指在大模型执行特定任务之前，先在大规模数据上进行自监督训练。这一阶段模型学习语言的基础知识，例如词汇、语法和常见的句子结构,帮助模型建立对语言的基本理解。</td> <td></td> </tr> <tr> <td>精调</td> <td><strong>指在预训练之后在特定任务的数据集上进行进一步训练</strong>,使得模型能够适应特定任务的需求，如文本分类、情感分析等。</td> <td></td> </tr> <tr> <td>prompt</td> <td><strong>prompt是指引导模型生成特定回应的输入文本</strong>，可以是一个问题、命令或任何其他形式的语句，用于激发模型按照某种特定方式响应。</td> <td></td> </tr> <tr> <td>instruction tuning</td> <td>指令调优是一种训练模型以更好地理解和遵循人类指令的方法。<strong>在模型训练中使用明确的指令</strong>，以提高其在执行特定任务时的准确性和效率。</td> <td></td> </tr> <tr> <td>chain-of-thought</td> <td>链式思考是指<strong>让模型在生成回答时展示其思考过程的方法</strong>。这种方式可以帮助理解模型是如何从问题到答案的过程，提高结果的可解释性，同时提高答案的准确性和质量。</td> <td></td> </tr> <tr> <td>RLHF</td> <td>通过人类反馈来进行强化学习的方法，帮助模型更好地理解适应人类的偏好。</td> <td></td> </tr> <tr> <td>hallucination</td> <td><strong>模型生成与现实不符或完全虚构的信息</strong>，这通常发生在模型对现实情况理解不足或处理能力有限时。</td> <td></td> </tr> <tr> <td>in-context learning</td> <td>模型能够根据给定的上下文信息来进行学习和预测，<strong>让模型根据上下文中的提示自我调整以适应新的任务或数据</strong>。</td> <td></td> </tr> <tr> <td>价值观对齐</td> <td>使模型的行为和决策与<strong>人类的伦理标准和价值观</strong>保持一致，确保模型的输出不会传播有害的内容，反映社会多元化的价值观，并尊重用户的隐私和偏好。</td> <td></td> </tr> <tr> <td>文本蕴含</td> <td>确定一个文本片段<strong>（前提）是否逻辑上蕴含或支持</strong>另一个文本片段<strong>（假设）</strong>的过程。如果从前提中可以推理出假设，则认为蕴含关系成立。</td> <td>✔️</td> </tr> <tr> <td>注意力机制</td> <td>是一种在神经网络中用于增强模型对特定部分输入数据的“关注”的技术，它允许模型在处理信息时更加集中于输入序列的某些重要部分</td> <td>✔️</td> </tr> <tr> <td>自然语言</td> <td>是人类用于日常沟通和表达的语言。</td> <td>✔️</td> </tr> <tr> <td>循环神经网络</td> <td>用于处理序列数据（如文本或时间序列数据）的神经网络，将先前的输出作为当前步骤输入的一部分，使它能够存储和利用历史信息更好地处理序列数据。</td> <td>✔️</td> </tr> <tr> <td>句法结构</td> <td>一个语言单位的语言成分及其句法关系组成的结构</td> <td></td> </tr> <tr> <td>依存结构</td> <td>一个语言单位的词汇间的依存关系组成的结构</td> <td></td> </tr> <tr> <td>EDU</td> <td>是构建更大语篇结构的基本组成部分，它们在语义上相对独立。通常包含了一个完整的主谓结构或一个独立的语篇成分。</td> <td></td> </tr> </tbody> </table> <h2 id=_6>简答题</h2> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-11.png alt="An image caption" style="width: 80%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <hr> <h3 id=_7>句法成分树</h3> <ul> <li>句法成分树中节点是什么、边代表什么意思</li> </ul> <p>句法结构：一个语言单位的<strong>语言成分</strong>及其<strong>句法关系</strong>组成的结构</p> <p>句法成分树用于表示句子的成分结构，其中树的根节点通常对应整个句⼦，⽽其他节点对应句⼦中的不同成分，如名词短语（NP）、动词短语（VP）、形容词短语（AP）等，边代表成分之间的各类关系。</p> <blockquote> <p>从S开始，化为NP、VP、AP，然后再化为pro verb det noun prep，最后化为词。</p> </blockquote> <hr> <h3 id=_8>依存树(图)</h3> <ul> <li>依存树(图)中节点是什么、边代表什么意思</li> </ul> <p>依存结构：一个语言单位的<strong>词汇间的依存关系</strong>组成的结构</p> <p>依存关系：词与词之间的二元非对称关系称为依存关系。</p> <p>依存树用于表示句子中单词之间的依存关系，其中节点代表单词，边代表单词之间的依存关系。</p> <blockquote> <p>知乎：句子的依存结构展示了一个单词依赖于另外一个单词（修饰或者是参数）。描述为从 head（被修饰的主题）用箭头指向 dependent （修饰语）。</p> </blockquote> <p>根据CoreNLP，一共有五十几种依存关系，举例如下：</p> <p>举例一： advmod 状语修饰</p> <p>句子：Where do you want to go later ? 分析：go 被 where 修饰 即advmod ( go -&gt; where )</p> <p>举例二：amod 形容词修饰</p> <p>句子：The cat is big. 分析：cat 被 big 修饰 即 amod ( cat -&gt; big )</p> <hr> <h3 id=rst>修辞结构理论(RST)</h3> <ul> <li>修辞结构理论(RST)是如何表示篇章的，其中基础语篇单位(Elementary DiscourseUnit/EDU)、核心nucleus和卫星satellite 指什么</li> </ul> <p>修辞结构理论认为，连贯的篇章由不同层次的修辞关系组成，并且可以表示为一种树形结构。在 RST Tree 中，每一个修辞关系都包括一个 Nucleus 和一个或多个 Satellite。</p> <p>通过描述各部分的修辞关系来分析篇章的结构和功能，大小不一的部分被称为结构段（text span）或者<strong>基础语篇单位(Elementary Discourse Unit/EDU)</strong>；text span由多个EDU组成</p> <p>核心是篇章最重要的部分，表示<strong>中心信息的单元，具有相对完整的语义</strong>。</p> <p>卫星是<strong>传达支撑信息的其他单元，用于补充说明核心部分</strong>，脱离核心的卫星部分通常是没有意义的。</p> <p>篇章分析的几个步骤：</p> <ol> <li>篇章分成基础语篇单位EDU</li> <li>判断每个EDU是核心还是卫星</li> <li>根据核心和卫星的特征来判断修辞关系，特征可以是词性，依存关系等</li> <li>构建篇章结构树</li> </ol> <hr> <h3 id=_9>注意力机制</h3> <ul> <li>注意力的运算过程</li> </ul> <blockquote> <p>参考资料 ：<a href=https://zhuanlan.zhihu.com/p/612036724>知乎：注意力机制详解</a></p> <p>自注意力机制本质是线性操作</p> </blockquote> <p>注意力机制就是加权聚焦的过程，即将一个句子中的每个词都乘以其注意力权重。</p> <p>如何计算注意力权重，公式如下所示，其中h为词向量矩阵，q为查询矩阵：</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-15.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <p>其中的s( )函数有多种计算方式：</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-16.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <p>接下来介绍最重要的自注意机制，其中的Q K V都是通过h线性变换得到的：</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-17.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>解释为什么叫自注意机制：<strong>查询向量</strong>也可以使用输入信息进行生成，而不是选择一个上述任务相关的查询向量。相当于模型读到输入信息后，根据输入信息本身决定当前最重要的信息。</p> </blockquote> <p>公式描述如下：</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-36.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-37.png alt="An image caption" style="width: 75%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-38.png alt="An image caption" style="width: 100%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <hr> <h3 id=_10>其他部分</h3> <ul> <li>Word2vec模型架构、训练词向量的原理</li> <li>前馈神经网络、卷积神经网络、循环神经网络、LSTM的模型架构</li> <li>Transformer的架构搞清楚:self-attention，cross-attention，multihead，layer-norm，残差，位置编码</li> <li>BERT的架构，预训练任务、什么是精调</li> <li>GPT-1的架构，预训练任务</li> </ul> <blockquote> <p>见重要模型部分内容即可</p> </blockquote> <h2 id=_11>算法题</h2> <h3 id=_12>分词算法</h3> <blockquote> <p>下面介绍的两种分词算法都是基于词典的方法，运行快速可以利用人工知识，但是没办法处理新词，还有其他的分词算法比如基于序列标注的算法，两者优缺点相反。</p> </blockquote> <ul> <li>AC自动机</li> </ul> <blockquote> <p>AC自动机是多模式串的字符匹配算法，能匹配出一个句子中与词库中词相同的词。</p> <p>比如：sherhsay -&gt; she he her say</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-34.png alt="An image caption" style="width: 80%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <hr> <ul> <li>最大匹配法</li> </ul> <p>可以将该分词算法的流程总结如下：</p> <ol> <li>基于词典构建前缀树</li> <li>为前缀树每个结点构建失配指针</li> <li>输入一个句子使用最大匹配法去匹配词典里的词</li> <li>根据匹配到的最长的词，得到分词结果</li> </ol> <hr> <ul> <li>最大概率法</li> </ul> <blockquote> <p>假设每个词的出现是条件独立的</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-4.png alt="An image caption" style="width: 80%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <h3 id=_13>词性标注算法</h3> <ul> <li>基于HMM的词性标注</li> </ul> <blockquote> <p>词性标注可以看作给定观察值（词），求隐状态（词性）的问题</p> <p><strong>发射概率（Emission Probability）</strong>表示在已知某个词性的情况下，观察到某个具体单词的概率。它告诉我们一个词语在特定词性下出现的可能性有多大。</p> <p><strong>转移概率（Transition Probability）</strong>表示了词性之间的转换概率。它告诉我们在一个句子中，一个词性转移到另一个词性的可能性有多大。</p> <p><strong>初始概率（Initial Probability）</strong>：这表示句子中第一个词性状态的概率分布。初始概率告诉模型在句子开始时，每种词性的可能性有多大。</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-5.png alt="An image caption" style="width: 80%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>维特比算法等同于计算无环有向图的最优(最大、最小)路径问题</p> </blockquote> <p>所以可以总结出算法的流程：</p> <ol> <li>得到训练数据</li> <li>利用训练数据和极大似然估计，计算隐马尔可夫模型的参数（三个概率）</li> <li>使用过Viterbi算法预测一个句子中每个词的词性</li> </ol> <h3 id=_14>生成模型解码算法</h3> <ul> <li>生成模型的解码算法: beam-search、Top-k采样、Top-p采样</li> </ul> <blockquote> <p>参考资料：<a href="https://www.bilibili.com/video/BV1Rb411M77p/?spm_id_from=333.337.search-card.all.click&vd_source=ae67b970bd4a0665fa92195df95aa1f3">Beam Search 束搜索与误差分析</a></p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-33.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>beamsearch相当于是greedy算法的扩展，num_beams=1时其实就是greedy算法。</p> <p>PPT上的解释为：贪心搜索即每次预测当前概率最大的元素，光束搜索(Beam Search)即每次保留最有可能的前k个元素，k为Beam 宽度</p> </blockquote> <p>Top-k 采样是一种生成策略，它在每个时间步骤中从词汇表中选择概率最高的k个单词或标记，然后从这k个选项中随机选择一个作为下一个词。这种方法可以增加多样性，因为它不会简单地选择最可能的单词，而是考虑了多个高概率选项。但是，它仍然受限于提前定义的k值。</p> <p>Top-p 采样是一种生成策略，它在每个时间步骤中按概率排名选择词汇表中的单词，直到累积概率达到或超过一个阈值p。这样，模型会考虑在累积概率阈值内的所有可能词汇，而不仅仅是前几个高概率选项。Top-p 采样可以提供更多的多样性，因为它允许模型选择概率分布中的较低概率词汇。</p> <blockquote> <p>Beam Search用于生成高质量的、连贯的序列</p> <p>Top-k和Top-p则用于在生成中引入随机性和多样性</p> </blockquote> <p>使用贪心策略会导致输出内容重复的问题怎么办？</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-48.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <h2 id=_15>设计题</h2> <ul> <li>会用神经网络构建关系抽取系统</li> <li>会用神经网络构建实体识别系统</li> <li>会用神经网络构建情感分析系统</li> <li>会用神经网络构建文本生成系统 （摘要系统、机器翻译系统、对话系统）</li> </ul> <h3 id=_16>完整流程</h3> <ul> <li> <p>问题定义：确定任务的具体目标和问题定义。明确需要解决的问题，例如，关系抽取是否要识别文本中的实体之间的关系，命名实体识别是否要标记文本中的命名实体等。</p> </li> <li> <p><strong>数据收集与清洗：</strong>收集和获取相关的文本数据，可以是标注好的数据集或原始文本数据。对数据进行预处理，包括文本清洗、去除停用词、词干化或词形还原等。处理后的数据应该是可用于模型训练的干净数据。</p> </li> <li> <p><strong>数据标注：</strong>如果任务需要标注数据（例如，为了训练监督学习模型），则进行数据标注，为数据添加相应的标签或注释。这对于关系抽取、命名实体识别等任务非常重要。</p> </li> <li> <p><strong>特征工程</strong>：根据任务的需求和数据的特点，进行特征工程，提取文本数据的相关特征。这可以包括<strong>词向量化、句子嵌入、TF-IDF特征提取</strong>等。</p> </li> <li> <p><strong>模型的选择和训练</strong></p> </li> <li> <p><strong>模型评估</strong>：使用验证集来评估模型性能，选择适当的评估指标，如精确度、召回率、F1分数、BLEU分数等，根据任务类型进行评估。</p> </li> <li> <p>模型微调：使用测试数据集来进一步评估模型的性能，可能需要进行微调以提高模型的泛化能力和稳定性。或者先预训练再微调。</p> </li> <li> <p><strong>部署与应用</strong>：将训练好的模型部署到生产环境中，以进行实际的NLP任务处理。这可以是一个API、移动应用、网站或其他应用程序。</p> </li> <li> <p>监控与维护：持续监控模型在实际应用中的性能，进行必要的维护和更新，以适应新的数据和需求。</p> </li> </ul> <h3 id=_17>评估指标</h3> <blockquote> <p>一般任务的的评估指标 : 精确度（Precision） 召回率（Recall）和F1分数（F1 Score）</p> <p>生成任务的评估指标 : BLEU（用于机器翻译）、ROUGE（用于文本摘要）</p> <p>BLEU和ROUGE都是内容重叠指标，其他指标也很重要：语义重叠指标，基于模型的指标，人工评估</p> <p>参考资料：<a href="https://baijiahao.baidu.com/s?id=1655137746278637231&wfr=spider&for=pc">评价指标：BLEU 和 ROUGE</a></p> </blockquote> <p>True Positive(真正, TP)：将正类预测为正类数. </p> <p>True Negative(真负, TN)：将负类预测为负类数. </p> <p>False Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error). </p> <p>False Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error). </p> <ul> <li>准确率：Acc = (tp + tn) / (tp + tn + fp + fn) 判断对了多少</li> <li>精确率：P = tp / (tp + fp) 在所有判断为正的例子中有多是真的正</li> <li>召回率：R = tp / (tp + fn) 真正的正例被判断出了多少</li> <li>F1分数：F1 = 2 * P * R / (P + R) 综合考虑了精确率和召回率的好坏，F1分数越高越好</li> </ul> <p>BLEU 和 ROUGE 是两个常用的评价指标，BLEU 根据精确率(Precision)衡量生成文本的质量，而 ROUGE 根据召回率(Recall)衡量生成文本的质量。</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-39.png alt="An image caption" style="width: 60%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>机器翻译的长度比较短时，BLEU 得分也会比较高，因此需要在 BLEU 分数乘上惩罚因子BP</p> </blockquote> <p>参考翻译的结果: a cat is on the table</p> <p>机器翻译的结果: there is a cat on the table</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-40.png alt="An image caption" style="width: 60%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <p>其他一些指标也要有所了解！</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-54.png alt="An image caption" style="width: 60%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-53.png alt="An image caption" style="width: 60%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <h3 id=_18>关系抽取</h3> <p>常见的关系抽取结果可以用SPO结构的三元组来表示，即（Subject, Predication, Object）如：中国的首都是北京==&gt; (中国，首都，北京)。思路一：分两个任务先抽取实体再判断实体之间的关系，思路二：同时进行实体抽取和关系抽取。</p> <ul> <li>思路一 ：BERT - FNN/Attention</li> </ul> <p>思路一很简单，把BERT的输入改成实体1、实体2、文本，输出改为多分类的全连接层即可。更复杂一点话呢可以像主题情感分析任务一样加入attension机制，以增加模型对实体1和实体2的理解和使用。之前做过实验了，可以说和主题情感分析一毛一样。</p> <blockquote> <p>在 j 老师的PPT中偶然看到了如下的模型图，感觉可以用，而且挺简单的哈哈哈...</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-35.png alt="An image caption" style="width: 80%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <ul> <li>思路二 ：BERT - CasRel</li> </ul> <blockquote> <p>《A Novel Cascade Binary Tagging Framework for Relational Triple Extraction》</p> <p>论文链接：https://aclanthology.org/2020.acl-main.136.pdf</p> <p>代码地址：https://github.com/weizhepei/CasRel</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-31.png alt="An image caption" style="width: 80%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <h3 id=_19>命名实体识别</h3> <ul> <li>BERT - BILSTM - CRF</li> </ul> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-23.png alt="An image caption" style="width: 90%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>CRF没有HMM中的严格独立性假设，能够考虑整个观测序列的信息，CRF可以选择更丰富的特征，不仅考虑标签本身，也考虑标签的转移得分。</p> </blockquote> <h3 id=_20>主题情感分类</h3> <ul> <li>BERT - Attention</li> </ul> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-32.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>把LSTM换成BERT</p> </blockquote> <p>一些三元组的例子：</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-41.png alt="An image caption" style="width: 70%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <h3 id=_21>自然语言生成系统</h3> <p>自然语言生成是隶属于自然语言处理的一个子领域，致力于构建能够自动生成连贯且有用的书面文本或口头文本的系统。比如机器翻译、文本摘要、对话系统等。</p> <p><strong>损失函数</strong>：最大化似然 -&gt; 最小化负对数似然 </p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-49.png alt="An image caption" style="width: 50%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>和最小化交叉熵损失非常类似，某些情况下可以等同，但是这里我还是使用PPT的说法最小化负对数似然 </p> </blockquote> <p><strong>优化损失函数</strong>：unlikehood training + likehood training</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-50.png alt="An image caption" style="width: 50%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>这个在PPT中被隐藏，仅作为拓展不要求掌握</p> </blockquote> <p><strong>Teacher forcing</strong>：在接下来的每个时间步骤，网络不是接收它自己之前的输出，而是接收实际序列中的前一个元素（即“正确的”输入），优势是可以加速训练过程并帮助网络更有效地学习正确的序列。然而，它也有一个潜在的缺点：当模型在实际使用时，可能会表现出不稳定，因为在训练期间它没有学习到如何从自己的错误中恢复。这个问题叫做<strong>暴露偏差</strong>。</p> <blockquote> <p>解决办法：以 𝑝 的概率，使用预测的词代替gold词作为模型的下一个输入，在训练过程中逐步提高 𝑝 </p> </blockquote> <p><strong>强化学习</strong>：给损失函数加一个reward</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-51.png alt="An image caption" style="width: 50%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-52.png alt="An image caption" style="width: 50%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>使用强化学习进行训练可以让模型学习出各种行为，但是学习过程不稳定</p> </blockquote> <h4 id=_22>翻译系统</h4> <blockquote> <p>直接使用transformer框架就好啦！这个本身是transformer的一种常见引用，不多赘述了。</p> </blockquote> <h4 id=_23>摘要系统</h4> <ul> <li>BERT - Transformer</li> </ul> <blockquote> <p>很牛的PEGASUS其实可以理解为Transformer - Transformer，思路差不多，不过设计了一个近似的无监督摘要任务，这样就不需要很多标注数据咯...</p> </blockquote> <p>文本摘要可分为抽取式摘要和生成式摘要：抽取式摘要从源文档中抽取关键句和关键词组成摘要，摘要信息全部来源于原。<strong>生成式摘要</strong>(NLG, Natural Language Generation)根据原文，允许生成新的词语、短语来组成摘要。生成式摘要也称为抽象式摘要。BERT两者都能实现。</p> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-20.png alt="An image caption" style="width: 60%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> <blockquote> <p>编码器用于提取重要的文本特征，解码器用于生成摘要。编码器使用预训练好的bert， 解码器直接使用transformer的解码器并随机初始化，用标注数据训练</p> </blockquote> <figure class=figure-image style="text-align: center;"> <img src=../pics/image-1.jpg alt="An image caption" style="width: 60%; display: block; margin: 0 auto; border: 2px solid #000080; border-radius: 10px; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.5);"> </figure> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../%E8%80%83%E8%AF%95%E8%AF%B4%E6%98%8E/ class="md-footer__link md-footer__link--prev" aria-label="上一页: 考试说明"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> 考试说明 </div> </div> </a> <a href=../../../%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%A4%A7%E4%BD%9C%E4%B8%9A/ class="md-footer__link md-footer__link--next" aria-label="下一页: 设计简陋的OS"> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> 设计简陋的OS </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2024 Whu-NS </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/Summu77 target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../../assets/javascripts/bundle.d7c377c4.min.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../assets/javascripts/custom.2340dcd7.min.js></script> </body> </html>